{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import os, gc, pickle, time, datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on this great kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:  # Exclude strings                    \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = df[col].max()\n",
    "            mn = df[col].min()\n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(df[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                df[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = df[col].fillna(0).astype(np.int64)\n",
    "            result = (df[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)    \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    return df, NAlist\n",
    "\n",
    "def extract_id_meter(df, building_id, meter):\n",
    "    temp = df[df['building_id'] == building_id]\n",
    "    temp = temp[temp['meter'] == meter]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_core_data(df):\n",
    "    # Check lossed Date  ####################################################################\n",
    "    id_list = []\n",
    "    meter_list = []\n",
    "    rows_list = []\n",
    "\n",
    "    for id_ in range(df['building_id'].nunique()):\n",
    "        for meter in range(4):\n",
    "            temp = extract_id_meter(df, id_, meter)\n",
    "            rows = temp.shape[0]\n",
    "            if rows not in [0, 8784]:\n",
    "                id_list.append(id_)\n",
    "                meter_list.append(meter)\n",
    "                rows_list.append(rows)\n",
    "\n",
    "    df_loss = pd.DataFrame({\n",
    "        'building_id': id_list,\n",
    "        'meter': meter_list,\n",
    "        'rows': rows_list\n",
    "    })\n",
    "    del id_list, meter_list, rows_list\n",
    "    \n",
    "    # Fill dropped Date\n",
    "    def fill_date(_df, building_id, meter):\n",
    "        temp = extract_id_meter(_df, building_id, meter)\n",
    "\n",
    "        dates_DF = pd.DataFrame(pd.date_range('2016-1-1', periods=366*24, freq='H'), columns=['Date'])\n",
    "        dates_DF['Date'] = dates_DF['Date'].apply(lambda x: x.strftime('%Y-%m-%d %T'))\n",
    "\n",
    "        temp = pd.merge(temp, dates_DF, how=\"outer\", left_on=['timestamp'], right_on=['Date'])\n",
    "        del temp['timestamp']\n",
    "        temp = temp.rename(columns={'Date': 'timestamp'})\n",
    "        temp['building_id'] = building_id\n",
    "        temp['meter'] = meter\n",
    "\n",
    "        temp = temp[temp['meter_reading'].isnull()]\n",
    "        _df = pd.concat([_df, temp], axis=0, ignore_index=True)\n",
    "\n",
    "        return _df\n",
    "\n",
    "    for _id, meter in zip(df_loss['building_id'], df_loss['meter']):\n",
    "        df = fill_date(df, _id, meter)\n",
    "        \n",
    "    # Interpolate    ####################################################################\n",
    "    for _id in range(df['building_id'].nunique()):\n",
    "        for meter in df['meter'].unique().tolist():\n",
    "            temp = extract_id_meter(df, _id, meter)\n",
    "            temp = temp.sort_values(by='timestamp')\n",
    "            \n",
    "            if temp.empty:\n",
    "                continue\n",
    "            # meter_readingが0のものを欠損として扱う\n",
    "            temp.loc[temp['meter_reading'] == 0, 'meter_reading'] = np.nan\n",
    "            # 欠損を内挿で埋める(あまり長い欠損は対象外)\n",
    "            temp['meter_reading'] = temp['meter_reading'].interpolate(limit_area='inside', limit=5)\n",
    "            df.loc[temp.index, 'meter_reading'] = temp.loc[temp.index, 'meter_reading']\n",
    "            \n",
    "    # Dropna    ####################################################################\n",
    "    df.dropna(inplace=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Weather Data\n",
    "def prep_weather_data(df):\n",
    "    # Drop Features  #####################################################################\n",
    "    drop_col = ['precip_depth_1_hr', 'sea_level_pressure', 'cloud_coverage']\n",
    "    df.drop(drop_col, axis=1, inplace=True)\n",
    "    \n",
    "    # Create Features per Site Id  #####################################################################\n",
    "    # Fillna(Interpolate)\n",
    "    for i in range(df['site_id'].nunique()):\n",
    "        temp = df[df['site_id'] == i]\n",
    "        temp = temp.sort_values(by='timestamp')\n",
    "\n",
    "        # Interpolation\n",
    "        cols = ['air_temperature', 'dew_temperature', 'wind_direction', 'wind_speed']\n",
    "        for c in cols:\n",
    "            temp[c] = temp[c].interpolate(limit_direction='both')\n",
    "            df.loc[temp.index, c] = temp.loc[temp.index, c]\n",
    "                \n",
    "                \n",
    "    # relative Hummd  #####################################################################\n",
    "    # https://soudan1.biglobe.ne.jp/qa5356721.html\n",
    "    a_temp = df['air_temperature'].values\n",
    "    d_temp = df['dew_temperature'].values\n",
    "    def SaturatedWaterVaporPressure(values):\n",
    "        return 6.11 * 10 ** (7.5 * values / (237.3 + values))\n",
    "    \n",
    "    a_temp = SaturatedWaterVaporPressure(a_temp)\n",
    "    d_temp = SaturatedWaterVaporPressure(d_temp)\n",
    "    \n",
    "    df['relative_hummd'] = d_temp / a_temp * 100\n",
    "    del a_temp, d_temp\n",
    "    \n",
    "    # Wind Direction  #####################################################################\n",
    "    df.loc[df['wind_direction'] == 65535, 'wind_direction'] = np.nan\n",
    "    df['wind_direction'] = np.radians(df['wind_direction'])\n",
    "    df['wind_direction_sin'] = np.sin(df['wind_direction'])\n",
    "    df['wind_direction_cos'] = np.cos(df['wind_direction'])\n",
    "    df['wind_direction_tan'] = np.tan(df['wind_direction'])\n",
    "    \n",
    "    df['wind_speed_sin'] = df['wind_speed'] * df['wind_direction_sin']\n",
    "    df['wind_speed_cos'] = df['wind_speed'] * df['wind_direction_cos']\n",
    "    \n",
    "    \n",
    "    # Create Features per Site Id  #####################################################################\n",
    "    for i in range(df['site_id'].nunique()):\n",
    "        temp = df[df['site_id'] == i]\n",
    "        temp = temp.sort_values(by='timestamp')\n",
    "    # Rolling\n",
    "        cols = ['air_temperature', 'dew_temperature', 'relative_hummd', 'wind_speed_sin', 'wind_speed_cos']\n",
    "        for c in cols:\n",
    "            for window in range(2, 5, 1):\n",
    "                colname = '{}_roll_{}_mean'.format(c, window)\n",
    "                temp[colname] = temp[c].rolling(window).mean()\n",
    "                df.loc[temp.index, colname] = temp.loc[temp.index, colname]\n",
    "                colname = '{}_roll_{}_sum'.format(c, window)\n",
    "                temp[colname] = temp[c].rolling(window).sum()\n",
    "                df.loc[temp.index, colname] = temp.loc[temp.index, colname]\n",
    "\n",
    "        # Shift\n",
    "        cols = ['air_temperature', 'dew_temperature', 'relative_hummd', 'wind_speed_sin', 'wind_speed_cos']\n",
    "        for c in cols:\n",
    "            for period in range(1, 3, 1):\n",
    "                colname = '{}_shift_{}'.format(c, period)\n",
    "                shifted = temp[c].shift(periods=period)\n",
    "                temp[colname] = temp[c] - shifted\n",
    "                df.loc[temp.index, colname] = temp.loc[temp.index, colname]\n",
    "                      \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingDataset:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        \n",
    "    def prep(self, df, df_weather, df_building, mode='train'):\n",
    "        \n",
    "        # Core Data Prep  #####################################################################\n",
    "#         if mode == 'train':\n",
    "#             df = prep_core_data(df)\n",
    "            \n",
    "        # Weather Data Prep  #####################################################################\n",
    "#         df_weather = prep_weather_data(df_weather)\n",
    "        \n",
    "        # merge data  #####################################################################\n",
    "        df = pd.merge(df, df_building, how=\"left\", on=[\"building_id\"])\n",
    "        df = pd.merge(df, df_weather, how='left', on=[\"site_id\", \"timestamp\"])\n",
    "        self.df, _ = reduce_mem_usage(df)\n",
    "        del df, df_weather, df_building\n",
    "        gc.collect()\n",
    "        \n",
    "        # Datetime  #####################################################################\n",
    "        self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])\n",
    "        self.df['month'] = self.df['timestamp'].dt.month.astype(np.uint8)\n",
    "        self.df['day'] = self.df['timestamp'].dt.day.astype(np.uint8)\n",
    "        self.df['hour'] = self.df['timestamp'].dt.hour.astype(np.uint8)\n",
    "        self.df['weekday'] = self.df['timestamp'].dt.weekday.astype(np.uint8)\n",
    "        # Sort Timestamp  #####################################################################\n",
    "        self.df = self.df.sort_values(by='timestamp', ascending=True).reset_index(drop=True)\n",
    "        del self.df['timestamp']\n",
    "        gc.collect()\n",
    "        \n",
    "        # Year Built  #####################################################################\n",
    "        self.df['year_built'] = self.df['year_built'] - 1900\n",
    "        \n",
    "        # square_feet  #####################################################################\n",
    "        self.df['square_feet'] = np.log(self.df['square_feet'])\n",
    "        \n",
    "        # LabelEncoder  #####################################################################\n",
    "        list_cols = ['primary_use']\n",
    "        if mode == 'train':\n",
    "            self.ce_oe = ce.OrdinalEncoder(cols=list_cols,handle_unknown='impute')\n",
    "            self.df = self.ce_oe.fit_transform(self.df)\n",
    "        elif mode == 'test':\n",
    "            self.df = self.ce_oe.transform(self.df)\n",
    "        \n",
    "        # Data Type  #####################################################################\n",
    "        # float32\n",
    "        cols = self.df.select_dtypes(np.float64).columns\n",
    "        for c in cols:\n",
    "            self.df[c] = self.df[c].astype(np.float32)\n",
    "        # category\n",
    "        cols = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"day\", \"weekday\", \"month\", \"meter\"]\n",
    "        for c in cols:\n",
    "            self.df[c] = self.df[c].astype('category')\n",
    "            \n",
    "        # sort row_id  #####################################################################\n",
    "        if mode == 'test':\n",
    "            self.df = self.df.sort_values(by='row_id').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, df, params, cv, num_boost_round, early_stopping_rounds, verbose, split=None):\n",
    "        self.y = np.log1p(df['meter_reading'])\n",
    "        self.x = df.drop(['meter_reading'], axis=1)\n",
    "        self.cv = cv\n",
    "        self.oof = 0.0\n",
    "        self.models = []\n",
    "        self.features = self.x.columns\n",
    "        \n",
    "        if split is None:\n",
    "            _cv = cv.split(self.x)\n",
    "        else:\n",
    "            _cv = cv.split(self.x, self.x[split])\n",
    "        \n",
    "        for i, (trn_idx, val_idx) in enumerate(_cv):\n",
    "            print('Fold {} Model Creating...'.format(i+1))\n",
    "            _start = time.time()\n",
    "\n",
    "            train_data = lgb.Dataset(self.x.iloc[trn_idx], label=self.y.iloc[trn_idx])\n",
    "            val_data = lgb.Dataset(self.x.iloc[val_idx], label=self.y.iloc[val_idx], reference=train_data)\n",
    "\n",
    "            model = lgb.train(params, \n",
    "                              train_data, \n",
    "                              num_boost_round=num_boost_round,\n",
    "                              valid_sets=(train_data, val_data),\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              verbose_eval=verbose)\n",
    "\n",
    "            y_pred = model.predict(self.x.iloc[val_idx], num_iteration=model.best_iteration)\n",
    "            error = np.sqrt(mean_squared_error(y_pred, self.y.iloc[val_idx]))\n",
    "            self.oof += error / cv.n_splits\n",
    "\n",
    "            print('Fold {}: {:.5f}'.format(i+1, error))\n",
    "\n",
    "            elapsedtime = time.time() - _start\n",
    "            print('Elapsed Time: {}'.format(str(datetime.timedelta(seconds=elapsedtime))))\n",
    "            print('')\n",
    "        \n",
    "            self.models.append(model)\n",
    "        print('OOF Error: {:.5f}'.format(self.oof))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def predict(self, df, step_size=500):\n",
    "        \n",
    "        if 'row_id' in df.columns:\n",
    "            df.drop('row_id', axis=1, inplace=True)\n",
    "        \n",
    "        if step_size is not None:\n",
    "            i=0\n",
    "            res=[]\n",
    "            for j in range(int(np.ceil(df.shape[0]/step_size))):\n",
    "                res.append(np.expm1(sum([model.predict(df.iloc[i:i+step_size], num_iteration=model.best_iteration) for model in self.models]) / self.cv.n_splits))\n",
    "                i+=step_size\n",
    "\n",
    "            res = np.concatenate(res)\n",
    "            \n",
    "        else:\n",
    "            res = np.zeros(len(df))\n",
    "            for model in self.models:\n",
    "                res += model.predict(df) / self.cv.n_splits\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        importance = np.zeros(len(self.features))\n",
    "        \n",
    "        for i in range(len(self.models)):\n",
    "            importance += self.models[i].feature_importance() / len(self.models)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': self.features,\n",
    "            'importance': importance\n",
    "        })\n",
    "        importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 20))\n",
    "        sns.barplot(x='importance', y='feature', data=importance_df)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading...\n",
      "Data Already...\n",
      "Fold 1 Model Creating...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\ttraining's rmse: 0.944782\tvalid_1's rmse: 0.945091\n",
      "[2000]\ttraining's rmse: 0.866382\tvalid_1's rmse: 0.867117\n",
      "[3000]\ttraining's rmse: 0.833906\tvalid_1's rmse: 0.834956\n",
      "[4000]\ttraining's rmse: 0.802003\tvalid_1's rmse: 0.80333\n",
      "[5000]\ttraining's rmse: 0.774049\tvalid_1's rmse: 0.775641\n",
      "[6000]\ttraining's rmse: 0.752955\tvalid_1's rmse: 0.754851\n",
      "[7000]\ttraining's rmse: 0.738338\tvalid_1's rmse: 0.740591\n",
      "[8000]\ttraining's rmse: 0.722361\tvalid_1's rmse: 0.725044\n",
      "[9000]\ttraining's rmse: 0.7092\tvalid_1's rmse: 0.712265\n",
      "[10000]\ttraining's rmse: 0.699254\tvalid_1's rmse: 0.702773\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 0.699254\tvalid_1's rmse: 0.702773\n",
      "Fold 1: 0.70277\n",
      "Elapsed Time: 1:34:10.503920\n",
      "\n",
      "Fold 2 Model Creating...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prep Train Data  #####################################################################\n",
    "print('Data Loading...')\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "df_weather_train = pd.read_csv(\"../input/weather_train.csv\")\n",
    "df_building = pd.read_csv(\"../input/building_metadata.csv\")\n",
    "\n",
    "data = PreprocessingDataset()\n",
    "data.prep(train, df_weather_train, df_building, mode='train')\n",
    "del train, df_weather_train, df_building\n",
    "print('Data Already...')\n",
    "\n",
    "# Config  #####################################################################\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'rmse'},\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.9\n",
    "}\n",
    "\n",
    "num_folds = 4\n",
    "cv = StratifiedKFold(num_folds, shuffle=True, random_state=42)\n",
    "num_boost_round = 10000\n",
    "early_stopping_rounds = 100\n",
    "verbose = 1000\n",
    "split = 'month'\n",
    "\n",
    "# Model Create  #####################################################################\n",
    "model = Trainer()\n",
    "_ = model.train(data.df, params, cv, num_boost_round, early_stopping_rounds, verbose, split)\n",
    "\n",
    "# Plot Feature Importances  #####################################################################\n",
    "model.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Chunksize ver\n",
    "chunk_size = 50000\n",
    "test_reader = pd.read_csv(\"../input/test.csv\", chunksize=chunk_size)\n",
    "df_weather_test = pd.read_csv(\"../input/weather_test.csv\")\n",
    "df_building = pd.read_csv(\"../input/building_metadata.csv\")\n",
    "\n",
    "pred_all = []\n",
    "\n",
    "for test in tqdm(test_reader):\n",
    "    data.prep(test, df_weather_test, df_building, mode='test')\n",
    "    pred = model.predict(data.df, step_size=None)\n",
    "    pred_all.append(pred)\n",
    "    \n",
    "pred_all = np.concatenate(pred_all)\n",
    "\n",
    "# Make Submission File\n",
    "sub = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sub[\"meter_reading\"] = pred\n",
    "today = datetime.datetime.now().strftime('%Y%m%d')\n",
    "sub.to_csv(\"../Output/submission_{}_oof_{:.3f}.csv\".format(today, model.oof), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "df_weather_test = pd.read_csv(\"../input/weather_test.csv\")\n",
    "df_building = pd.read_csv(\"../input/building_metadata.csv\")\n",
    "\n",
    "data.prep(test, df_weather_test, df_building, mode='test')\n",
    "del test, df_weather_test, df_building\n",
    "gc.collect()\n",
    "\n",
    "pred = model.predict(data.df)\n",
    "\n",
    "# Make Submission File\n",
    "sub = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sub[\"meter_reading\"] = pred\n",
    "today = datetime.datetime.now().strftime('%Y%m%d')\n",
    "sub.to_csv(\"../Output/submission_{}_oof_{:.3f}.csv\".format(today, model.oof), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
