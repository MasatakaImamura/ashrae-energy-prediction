{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import os, gc, pickle, time, datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on this great kernel https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:  # Exclude strings                    \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = df[col].max()\n",
    "            mn = df[col].min()\n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(df[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                df[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = df[col].fillna(0).astype(np.int64)\n",
    "            result = (df[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        df[col] = df[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        df[col] = df[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        df[col] = df[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)    \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            \n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    return df, NAlist\n",
    "\n",
    "def extract_id_meter(df, building_id, meter):\n",
    "    temp = df[df['building_id'] == building_id]\n",
    "    temp = temp[temp['meter'] == meter]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_core_data(df):\n",
    "    # Check lossed Date  ####################################################################\n",
    "    id_list = []\n",
    "    meter_list = []\n",
    "    rows_list = []\n",
    "\n",
    "    for id_ in range(df['building_id'].nunique()):\n",
    "        for meter in range(4):\n",
    "            temp = extract_id_meter(df, id_, meter)\n",
    "            rows = temp.shape[0]\n",
    "            if rows not in [0, 8784]:\n",
    "                id_list.append(id_)\n",
    "                meter_list.append(meter)\n",
    "                rows_list.append(rows)\n",
    "\n",
    "    df_loss = pd.DataFrame({\n",
    "        'building_id': id_list,\n",
    "        'meter': meter_list,\n",
    "        'rows': rows_list\n",
    "    })\n",
    "    del id_list, meter_list, rows_list\n",
    "    \n",
    "    # Fill dropped Date\n",
    "    def fill_date(_df, building_id, meter):\n",
    "        temp = extract_id_meter(_df, building_id, meter)\n",
    "\n",
    "        dates_DF = pd.DataFrame(pd.date_range('2016-1-1', periods=366*24, freq='H'), columns=['Date'])\n",
    "        dates_DF['Date'] = dates_DF['Date'].apply(lambda x: x.strftime('%Y-%m-%d %T'))\n",
    "\n",
    "        temp = pd.merge(temp, dates_DF, how=\"outer\", left_on=['timestamp'], right_on=['Date'])\n",
    "        del temp['timestamp']\n",
    "        temp = temp.rename(columns={'Date': 'timestamp'})\n",
    "        temp['building_id'] = building_id\n",
    "        temp['meter'] = meter\n",
    "\n",
    "        temp = temp[temp['meter_reading'].isnull()]\n",
    "        _df = pd.concat([_df, temp], axis=0, ignore_index=True)\n",
    "\n",
    "        return _df\n",
    "\n",
    "    for _id, meter in zip(df_loss['building_id'], df_loss['meter']):\n",
    "        df = fill_date(df, _id, meter)\n",
    "        \n",
    "    # Interpolate    ####################################################################\n",
    "    for _id in range(df['building_id'].nunique()):\n",
    "        for meter in df['meter'].unique().tolist():\n",
    "            # building_id, meterで抽出\n",
    "            temp = extract_id_meter(df, _id, meter)\n",
    "            temp = temp.sort_values(by='timestamp')\n",
    "            \n",
    "            if temp.empty:\n",
    "                continue\n",
    "            # meter_readingが0のものを欠損として扱う\n",
    "            temp.loc[temp['meter_reading'] == 0, 'meter_reading'] = np.nan\n",
    "            # 欠損を内挿で埋める\n",
    "            temp['meter_reading'] = temp['meter_reading'].interpolate(limit_area='inside', limit=5)\n",
    "            df.loc[temp.index, 'meter_reading'] = temp.loc[temp.index, 'meter_reading']\n",
    "            \n",
    "    # Dropna    ####################################################################\n",
    "    df.dropna(inplace=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Weather Data\n",
    "def prep_weather_data(df):\n",
    "    # Drop Features  #####################################################################\n",
    "    drop_col = ['precip_depth_1_hr', 'sea_level_pressure', 'cloud_coverage']\n",
    "    df.drop(drop_col, axis=1, inplace=True)\n",
    "    \n",
    "    # Create Features per Site Id  #####################################################################\n",
    "    # Fillna(Interpolate)\n",
    "    for i in range(df['site_id'].nunique()):\n",
    "        temp = df[df['site_id'] == i]\n",
    "        temp = temp.sort_values(by='timestamp')\n",
    "\n",
    "        # Interpolation\n",
    "        cols = ['air_temperature', 'dew_temperature', 'wind_direction', 'wind_speed']\n",
    "        for c in cols:\n",
    "            temp[c] = temp[c].interpolate(limit_direction='both')\n",
    "            df.loc[temp.index, c] = temp.loc[temp.index, c]\n",
    "                \n",
    "                \n",
    "    # relative Hummd  #####################################################################\n",
    "    # https://soudan1.biglobe.ne.jp/qa5356721.html\n",
    "    a_temp = df['air_temperature'].values\n",
    "    d_temp = df['dew_temperature'].values\n",
    "    def SaturatedWaterVaporPressure(values):\n",
    "        return 6.11 * 10 ** (7.5 * values / (237.3 + values))\n",
    "    \n",
    "    a_temp = SaturatedWaterVaporPressure(a_temp)\n",
    "    d_temp = SaturatedWaterVaporPressure(d_temp)\n",
    "    \n",
    "    df['relative_hummd'] = d_temp / a_temp * 100\n",
    "    del a_temp, d_temp\n",
    "    \n",
    "    # Wind Direction  #####################################################################\n",
    "    df.loc[df['wind_direction'] == 65535, 'wind_direction'] = np.nan\n",
    "    df['wind_direction'] = np.radians(df['wind_direction'])\n",
    "    df['wind_direction_sin'] = np.sin(df['wind_direction'])\n",
    "    df['wind_direction_cos'] = np.cos(df['wind_direction'])\n",
    "    df['wind_direction_tan'] = np.tan(df['wind_direction'])\n",
    "    \n",
    "    df['wind_speed_sin'] = df['wind_speed'] * df['wind_direction_sin']\n",
    "    df['wind_speed_cos'] = df['wind_speed'] * df['wind_direction_cos']\n",
    "    \n",
    "    \n",
    "    # Create Features per Site Id  #####################################################################\n",
    "    for i in range(df['site_id'].nunique()):\n",
    "        temp = df[df['site_id'] == i]\n",
    "        temp = temp.sort_values(by='timestamp')\n",
    "    # Rolling\n",
    "        cols = ['air_temperature', 'dew_temperature', 'relative_hummd', 'wind_speed_sin', 'wind_speed_cos']\n",
    "        for c in cols:\n",
    "            for window in range(2, 5, 1):\n",
    "                colname = '{}_roll_{}_mean'.format(c, window)\n",
    "                temp[colname] = temp[c].rolling(window).mean()\n",
    "                df.loc[temp.index, colname] = temp.loc[temp.index, colname]\n",
    "                colname = '{}_roll_{}_sum'.format(c, window)\n",
    "                temp[colname] = temp[c].rolling(window).sum()\n",
    "                df.loc[temp.index, colname] = temp.loc[temp.index, colname]\n",
    "\n",
    "        # Shift\n",
    "        cols = ['air_temperature', 'dew_temperature', 'relative_hummd', 'wind_speed_sin', 'wind_speed_cos']\n",
    "        for c in cols:\n",
    "            for period in range(1, 3, 1):\n",
    "                colname = '{}_shift_{}'.format(c, period)\n",
    "                shifted = temp[c].shift(periods=period)\n",
    "                temp[colname] = temp[c] - shifted\n",
    "                df.loc[temp.index, colname] = temp.loc[temp.index, colname]\n",
    "                      \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingDataset:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        \n",
    "    def prep(self, df, df_weather, df_building, mode='train'):\n",
    "        \n",
    "        # Core Data Prep  #####################################################################\n",
    "        if mode == 'train':\n",
    "            df = prep_core_data(df)\n",
    "            \n",
    "        # Weather Data Prep  #####################################################################\n",
    "        df_weather = prep_weather_data(df_weather)\n",
    "        \n",
    "        # merge data  #####################################################################\n",
    "        df = pd.merge(df, df_building, how=\"left\", on=[\"building_id\"])\n",
    "        df = pd.merge(df, df_weather, how='left', on=[\"site_id\", \"timestamp\"])\n",
    "        self.df, _ = reduce_mem_usage(df)\n",
    "        del df, df_weather, df_building\n",
    "        gc.collect()\n",
    "        \n",
    "        # Datetime  #####################################################################\n",
    "        self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])\n",
    "        self.df['month'] = self.df['timestamp'].dt.month.astype(np.uint8)\n",
    "        self.df['day'] = self.df['timestamp'].dt.day.astype(np.uint8)\n",
    "        self.df['hour'] = self.df['timestamp'].dt.hour.astype(np.uint8)\n",
    "        self.df['weekday'] = self.df['timestamp'].dt.weekday.astype(np.uint8)\n",
    "        # Sort Timestamp  #####################################################################\n",
    "        self.df = self.df.sort_values(by='timestamp', ascending=True).reset_index(drop=True)\n",
    "        del self.df['timestamp']\n",
    "        gc.collect()\n",
    "        \n",
    "        # Year Built  #####################################################################\n",
    "        self.df['year_built'] = self.df['year_built'] - 1900\n",
    "        \n",
    "        # square_feet  #####################################################################\n",
    "        self.df['square_feet'] = np.log(self.df['square_feet'])\n",
    "        \n",
    "        # LabelEncoder  #####################################################################\n",
    "        list_cols = ['primary_use']\n",
    "        if mode == 'train':\n",
    "            self.ce_oe = ce.OrdinalEncoder(cols=list_cols,handle_unknown='impute')\n",
    "            self.df = self.ce_oe.fit_transform(self.df)\n",
    "        elif mode == 'test':\n",
    "            self.df = self.ce_oe.transform(self.df)\n",
    "        \n",
    "        # Data Type  #####################################################################\n",
    "        # float32\n",
    "        cols = self.df.select_dtypes(np.float64).columns\n",
    "        for c in cols:\n",
    "            self.df[c] = self.df[c].astype(np.float32)\n",
    "        # category\n",
    "        cols = [\"site_id\", \"building_id\", \"primary_use\", \"hour\", \"day\", \"weekday\", \"month\", \"meter\"]\n",
    "        for c in cols:\n",
    "            self.df[c] = self.df[c].astype('category')\n",
    "            \n",
    "        # sort row_id  #####################################################################\n",
    "        if mode == 'test':\n",
    "            self.df = self.df.sort_values(by='row_id').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, df, params, cv, num_boost_round, early_stopping_rounds, verbose, split=None):\n",
    "        self.y = np.log1p(df['meter_reading'])\n",
    "        self.x = df.drop(['meter_reading'], axis=1)\n",
    "        self.cv = cv\n",
    "        self.oof = 0.0\n",
    "        self.models = []\n",
    "        self.features = self.x.columns\n",
    "        \n",
    "        if split is None:\n",
    "            _cv = cv.split(self.x)\n",
    "        else:\n",
    "            _cv = cv.split(self.x, self.x[split])\n",
    "        \n",
    "        for i, (trn_idx, val_idx) in enumerate(_cv):\n",
    "            print('Fold {} Model Creating...'.format(i+1))\n",
    "            _start = time.time()\n",
    "\n",
    "            train_data = lgb.Dataset(self.x.iloc[trn_idx], label=self.y.iloc[trn_idx])\n",
    "            val_data = lgb.Dataset(self.x.iloc[val_idx], label=self.y.iloc[val_idx], reference=train_data)\n",
    "\n",
    "            model = lgb.train(params, \n",
    "                              train_data, \n",
    "                              num_boost_round=num_boost_round,\n",
    "                              valid_sets=(train_data, val_data),\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              verbose_eval=verbose)\n",
    "\n",
    "            y_pred = model.predict(self.x.iloc[val_idx], num_iteration=model.best_iteration)\n",
    "            error = np.sqrt(mean_squared_error(y_pred, self.y.iloc[val_idx]))\n",
    "            self.oof += error / cv.n_splits\n",
    "\n",
    "            print('Fold {}: {:.5f}'.format(i+1, error))\n",
    "\n",
    "            elapsedtime = time.time() - _start\n",
    "            print('Elapsed Time: {}'.format(str(datetime.timedelta(seconds=elapsedtime))))\n",
    "            print('')\n",
    "        \n",
    "            self.models.append(model)\n",
    "        print('OOF Error: {:.5f}'.format(self.oof))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def predict(self, df, step_size=500):\n",
    "        \n",
    "        if 'row_id' in df.columns:\n",
    "            df.drop('row_id', axis=1, inplace=True)\n",
    "            \n",
    "        i=0\n",
    "        res=[]\n",
    "        for j in range(int(np.ceil(df.shape[0]/step_size))):\n",
    "            res.append(np.expm1(sum([model.predict(df.iloc[i:i+step_size], num_iteration=model.best_iteration) for model in self.models]) / self.cv.n_splits))\n",
    "            i+=step_size\n",
    "            \n",
    "        res = np.concatenate(res)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        importance = np.zeros(len(self.features))\n",
    "        \n",
    "        for i in range(len(self.models)):\n",
    "            importance += self.models[i].feature_importance() / len(self.models)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': self.features,\n",
    "            'importance': importance\n",
    "        })\n",
    "        importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 20))\n",
    "        sns.barplot(x='importance', y='feature', data=importance_df)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading...\n",
      "Data Already...\n",
      "Fold 1 Model Creating...\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prep Train Data  #####################################################################\n",
    "print('Data Loading...')\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "df_weather_train = pd.read_csv(\"../input/weather_train.csv\")\n",
    "df_building = pd.read_csv(\"../input/building_metadata.csv\")\n",
    "\n",
    "# Sampling\n",
    "train = train.sample(frac=0.01, random_state=SEED)\n",
    "\n",
    "data = PreprocessingDataset()\n",
    "data.prep(train, df_weather_train, df_building, mode='train')\n",
    "del train, df_weather_train, df_building\n",
    "print('Data Already...')\n",
    "\n",
    "data.df.head(100).to_csv('../Output/Prep_train.csv', index=False)\n",
    "\n",
    "# Config  #####################################################################\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'rmse'},\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.9\n",
    "}\n",
    "\n",
    "num_folds = 4\n",
    "cv = StratifiedKFold(num_folds, shuffle=True, random_state=42)\n",
    "num_boost_round = 6000\n",
    "early_stopping_rounds = 100\n",
    "verbose = 1000\n",
    "split = 'building_id'\n",
    "\n",
    "# Model Create  #####################################################################\n",
    "model = Trainer()\n",
    "_ = model.train(data.df, params, cv, num_boost_round, early_stopping_rounds, verbose, split)\n",
    "\n",
    "# Plot Feature Importances  #####################################################################\n",
    "model.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunksize ver\n",
    "\n",
    "chunk_size = 5000\n",
    "test_reader = pd.read_csv(\"../input/test.csv\", chunksize=chunk_size)\n",
    "df_weather_test = pd.read_csv(\"../input/weather_test.csv\")\n",
    "df_building = pd.read_csv(\"../input/building_metadata.csv\")\n",
    "\n",
    "pred_all = []\n",
    "\n",
    "for test in tqdm(test_reader):\n",
    "    data.prep(test, df_weather_test, df_building, mode='test')\n",
    "    pred = model.predict(data.df)\n",
    "    pred_all.append(pred)\n",
    "    \n",
    "pred_all = np.concatenate(pred_all)\n",
    "\n",
    "# Make Submission File\n",
    "sub = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sub[\"meter_reading\"] = pred\n",
    "today = datetime.datetime.now().strftime('%Y%m%d')\n",
    "sub.to_csv(\"../Output/submission_{}_oof_{:.3f}.csv\".format(today, model.oof), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (50, 41697600) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ec3dcb8201de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_building\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input/building_metadata.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_weather_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_building\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_weather_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_building\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c08079ed20ab>\u001b[0m in \u001b[0;36mprep\u001b[1;34m(self, df, df_weather, df_building, mode)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# merge data  #####################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_building\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"building_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_weather\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"site_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_weather\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_building\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\kaggle\\ashrae~1\\venv~1\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\kaggle\\ashrae~1\\venv~1\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    656\u001b[0m             \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m         )\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\kaggle\\ashrae~1\\venv~1\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   2052\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2053\u001b[0m             b = make_block(\n\u001b[1;32m-> 2054\u001b[1;33m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2055\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2056\u001b[0m             )\n",
      "\u001b[1;32md:\\python\\kaggle\\ashrae~1\\venv~1\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m    251\u001b[0m     to_concat = [\n\u001b[0;32m    252\u001b[0m         \u001b[0mju\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     ]\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\kaggle\\ashrae~1\\venv~1\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m     to_concat = [\n\u001b[0;32m    252\u001b[0m         \u001b[0mju\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     ]\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\kaggle\\ashrae~1\\venv~1\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[1;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\kaggle\\ashrae~1\\venv~1\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[0;32m   1714\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1716\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (50, 41697600) and data type float64"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "df_weather_test = pd.read_csv(\"../input/weather_test.csv\")\n",
    "df_building = pd.read_csv(\"../input/building_metadata.csv\")\n",
    "\n",
    "data.prep(test, df_weather_test, df_building, mode='test')\n",
    "del test, df_weather_test, df_building\n",
    "gc.collect()\n",
    "\n",
    "pred = model.predict(data.df)\n",
    "\n",
    "# Make Submission File\n",
    "sub = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sub[\"meter_reading\"] = pred\n",
    "today = datetime.datetime.now().strftime('%Y%m%d')\n",
    "sub.to_csv(\"../Output/submission_{}_oof_{:.3f}.csv\".format(today, model.oof), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id      0\n",
      "meter            0\n",
      "timestamp        0\n",
      "meter_reading    0\n",
      "dtype: int64\n",
      "(20216100, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\kaggle\\ashrae~1\\venv~1\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "print(train.isnull().sum())\n",
    "print(train.shape)\n",
    "\n",
    "train = prep_core_data(train)\n",
    "\n",
    "print(train.isnull().sum())\n",
    "print(train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a640d69fa2747f5b13beee0ce71cfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_train = train.copy().reset_index(drop=True)\n",
    "\n",
    "for _id in tqdm(range(_train['building_id'].nunique())):\n",
    "    for meter in _train['meter'].unique().tolist():\n",
    "\n",
    "        temp = _train[_train['building_id'] == _id]\n",
    "        temp = temp[temp['meter'] == meter]\n",
    "        temp = temp.sort_values(by='timestamp')\n",
    "        \n",
    "        if temp.empty:\n",
    "            continue\n",
    "\n",
    "        temp['meter_reading'] = temp['meter_reading'].interpolate(limit_area='inside')\n",
    "        _train.loc[temp.index, 'meter_reading'] = temp.loc[temp.index, 'meter_reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id           0\n",
      "meter                 0\n",
      "meter_reading    186582\n",
      "timestamp             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "building_id           0\n",
       "meter                 0\n",
       "meter_reading    689820\n",
       "timestamp             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = _train[_train['building_id'] == 7]\n",
    "a = a[a['meter'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.sort_values(by='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6327470</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4938.61</td>\n",
       "      <td>2016-04-28 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6329814</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5149.66</td>\n",
       "      <td>2016-04-28 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6332161</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5107.45</td>\n",
       "      <td>2016-04-28 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6334506</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4854.19</td>\n",
       "      <td>2016-04-28 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6336851</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4558.72</td>\n",
       "      <td>2016-04-28 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6339197</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4769.77</td>\n",
       "      <td>2016-04-28 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6341541</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4643.14</td>\n",
       "      <td>2016-04-28 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6343887</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4347.67</td>\n",
       "      <td>2016-04-28 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6346230</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3630.09</td>\n",
       "      <td>2016-04-28 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6348574</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2574.83</td>\n",
       "      <td>2016-04-28 22:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         building_id  meter  meter_reading            timestamp\n",
       "6327470            7      1        4938.61  2016-04-28 13:00:00\n",
       "6329814            7      1        5149.66  2016-04-28 14:00:00\n",
       "6332161            7      1        5107.45  2016-04-28 15:00:00\n",
       "6334506            7      1        4854.19  2016-04-28 16:00:00\n",
       "6336851            7      1        4558.72  2016-04-28 17:00:00\n",
       "6339197            7      1        4769.77  2016-04-28 18:00:00\n",
       "6341541            7      1        4643.14  2016-04-28 19:00:00\n",
       "6343887            7      1        4347.67  2016-04-28 20:00:00\n",
       "6346230            7      1        3630.09  2016-04-28 21:00:00\n",
       "6348574            7      1        2574.83  2016-04-28 22:00:00"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1420:1430]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
