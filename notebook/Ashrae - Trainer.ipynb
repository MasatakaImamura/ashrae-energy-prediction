{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imamuramasataka/Documents/kaggle/ashrae-energy-prediction/.venv/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "myfavouritenumber = 0\n",
    "seed = myfavouritenumber\n",
    "random.seed(seed)\n",
    "\n",
    "today = datetime.datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "pd.set_option('max_rows', 9999)\n",
    "pd.set_option('max_columns', 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half_Half_LightGBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, importance_df, model_type='lgb', use_feature_num=None):\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        if use_feature_num is not None:\n",
    "            self.features = importance_df['feature'][:use_feature_num].tolist()\n",
    "        else:\n",
    "            self.features = None\n",
    "            \n",
    "    def train_half(self, X_train, y_train, params, num_boost_round, early_stopping_rounds, verbose=200):\n",
    "        \n",
    "        if self.features is None:\n",
    "            self.features = X_train.columns\n",
    "            \n",
    "        self.features = [c for c in self.features if c not in ['M']]\n",
    "            \n",
    "        self.X_train = X_train[self.features]\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        if self.model_type == 'lgb':\n",
    "            print('LightGBM Model Creating...')\n",
    "            d_half_1 = lgb.Dataset(self.X_train[:int(self.X_train.shape[0] / 2)], \n",
    "                                   label=self.y_train[:int(X_train.shape[0] / 2)])\n",
    "            print(d_half_1)\n",
    "            print(type(d_half_1))\n",
    "            \n",
    "            d_half_2 = lgb.Dataset(self.X_train[int(self.X_train.shape[0] / 2):], \n",
    "                                   label=self.y_train[int(X_train.shape[0] / 2):])\n",
    "\n",
    "            print(\"Building model with first half and validating on second half:\")\n",
    "            self.model_1 = lgb.train(params, train_set=d_half_1, num_boost_round=num_boost_round, \n",
    "                                     valid_sets=[d_half_1, d_half_2], verbose_eval=verbose, \n",
    "                                     early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            print('')\n",
    "            print(\"Building model with second half and validating on first half:\")\n",
    "            self.model_2 = lgb.train(params, train_set=d_half_2, num_boost_round=num_boost_round, \n",
    "                                     valid_sets=[d_half_2, d_half_1], verbose_eval=verbose, \n",
    "                                     early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "        elif self.model_type == 'cat':\n",
    "            print('CatBoost Model Creating...')\n",
    "            cat_features_index = np.where(self.X_train.dtypes == 'category')[0]\n",
    "            d_half_1 = Pool(self.X_train[:int(self.X_train.shape[0] / 2)], \n",
    "                            label=self.y_train[:int(X_train.shape[0] / 2)],\n",
    "                            cat_features=cat_features_index)\n",
    "            d_half_2 = Pool(self.X_train[int(self.X_train.shape[0] / 2):], \n",
    "                            label=self.y_train[int(X_train.shape[0] / 2):],\n",
    "                            cat_features=cat_features_index)\n",
    "            \n",
    "            params['iterations'] = num_boost_round\n",
    "            print(\"Building model with first half and validating on second half:\")\n",
    "            model_1 = CatBoostRegressor(**params)\n",
    "            model_1.fit(d_half_1, eval_set=d_half_2, use_best_model=True,\n",
    "                        early_stopping_rounds=early_stopping_rounds,\n",
    "                        verbose=verbose)\n",
    "            \n",
    "            print('')\n",
    "            print(\"Building model with second half and validating on first half:\")\n",
    "            model_2 = CatBoostRegressor(**params)\n",
    "            model_2.fit(d_half_2, eval_set=d_half_1, use_best_model=True,\n",
    "                        early_stopping_rounds=early_stopping_rounds,\n",
    "                        verbose=verbose)\n",
    "            \n",
    "        \n",
    "        return (self.model_1, self.model_2)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        df_fimp_1 = pd.DataFrame()\n",
    "        df_fimp_1[\"feature\"] = self.X_train.columns.values\n",
    "        if self.model_type == 'lgb':\n",
    "            df_fimp_1[\"importance\"] = self.model_1.feature_importance()\n",
    "        elif self.model_type == 'cat':\n",
    "            df_fimp_1[\"importance\"] = self.model_1.get_feature_importance()\n",
    "        df_fimp_1[\"half\"] = 1\n",
    "\n",
    "        df_fimp_2 = pd.DataFrame()\n",
    "        df_fimp_2[\"feature\"] = self.X_train.columns.values\n",
    "        if self.model_type == 'lgb':\n",
    "            df_fimp_2[\"importance\"] = self.model_2.feature_importance()\n",
    "        elif self.model_type == 'cat':\n",
    "            df_fimp_2[\"importance\"] = self.model_2.get_feature_importance()\n",
    "        df_fimp_2[\"half\"] = 2\n",
    "\n",
    "        df_fimp = pd.concat([df_fimp_1, df_fimp_2], axis=0)\n",
    "\n",
    "        plt.figure(figsize=(14, int(len(self.X_train.columns) * 0.3)), facecolor='w')\n",
    "        sns.barplot(x=\"importance\", y=\"feature\", data=df_fimp.sort_values(by=\"importance\", ascending=False))\n",
    "        plt.title(\"LightGBM Feature Importance\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        del df_fimp_1, df_fimp_2, df_fimp\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config - LightGBM\n",
    "train_pkl_path = '../input/prep_train_20191125.pkl'\n",
    "test_pkl_path = '../input/prep_test_20191125.pkl'\n",
    "importance_path = '../Importance/importance_20191125.csv'\n",
    "model_type = 'lgb'\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbrt',\n",
    "    'metric': 'rmse',\n",
    "    'n_jobs': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_bin': 255,\n",
    "    'bagging_fraction': 0.42523401307460185,\n",
    "    'bagging_freq': 6,\n",
    "    'colsample_bytree': 0.7187743617504782,\n",
    "    'feature_fraction': 0.8822412268247286,\n",
    "    'max_depth': 10,\n",
    "    'min_data_in_leaf': 42,\n",
    "    'num_leaves': 279,\n",
    "    'reg_lambda': 114.8060332041216,\n",
    "    'subsample': 0.8631504025541011,\n",
    "    'verbose': -1,\n",
    "    'seed': 42\n",
    "    }\n",
    "\n",
    "num_boost_round = 10000\n",
    "early_stopping_rounds = 500\n",
    "num_feature = 40\n",
    "verbose = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Model Creating...\n",
      "<lightgbm.basic.Dataset object at 0x1203a8cd0>\n",
      "<class 'lightgbm.basic.Dataset'>\n",
      "Building model with first half and validating on second half:\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-24900db1207f>\u001b[0m in \u001b[0;36mtrain_half\u001b[0;34m(self, X_train, y_train, params, num_boost_round, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m     31\u001b[0m             self.model_1 = lgb.train(params, train_set=d_half_1, num_boost_round=num_boost_round, \n\u001b[1;32m     32\u001b[0m                                      \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_half_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_half_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                      early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/kaggle/ashrae-energy-prediction/.venv/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/kaggle/ashrae-energy-prediction/.venv/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1924\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1925\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1927\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training\n",
    "# Data Loading\n",
    "with open(train_pkl_path, 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "importance_df = pd.read_csv(importance_path)\n",
    "# Train Model\n",
    "trainer = Trainer(importance_df=importance_df, model_type=model_type, use_feature_num=num_feature)\n",
    "models = trainer.train_half(train[0], train[1], params, num_boost_round, early_stopping_rounds, verbose)\n",
    "\n",
    "del train, importance_df\n",
    "gc.collect()\n",
    "\n",
    "with open(f'../Model/model_{today}_{trainer.model_type}_half.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.get_feature_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing test data \n",
    "\n",
    "Preparing test data with same features as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predict & Submission\n",
    "# Load Models from Pickle\n",
    "with open('../Model/model_20191120_lgb_half.pkl', 'rb') as f:\n",
    "    models = pickle.load(f)\n",
    "importance_df = pd.read_csv(importance_path)\n",
    "trainer = Trainer(importance_df=importance_df, model_type=model_type, use_feature_num=num_feature)\n",
    "trainer.features = [c for c in trainer.features if c not in ['M']]\n",
    "\n",
    "# Data Loading\n",
    "with open(test_pkl_path, 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_test = test[0]\n",
    "row_id = test[1]\n",
    "del test\n",
    "gc.collect()\n",
    "    \n",
    "if num_feature is not None:\n",
    "    X_test = X_test[trainer.features]\n",
    "    gc.collect()\n",
    "\n",
    "# Set Predict values\n",
    "pred = np.zeros(len(row_id))\n",
    "\n",
    "# Prediction\n",
    "for model in models:\n",
    "    if trainer.model_type == 'lgb':\n",
    "        pred += np.expm1(model.predict(X_test, num_iteration=model.best_iteration)) / int(len(models))\n",
    "    elif trainer.model_type == 'cat':\n",
    "        pred += np.expm1(model.predict(X_test)) / int(len(models))\n",
    "    \n",
    "today = datetime.datetime.now().strftime('%Y%m%d')\n",
    "submission = pd.DataFrame({\"row_id\": row_id, \"meter_reading\": np.clip(pred, 0, a_max=None)})\n",
    "submission.to_csv(f\"../Output/submission_from_nb_{trainer.model_type}_{today}.csv\", index=False)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_test, row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
